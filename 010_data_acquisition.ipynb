{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2e5b07-f04f-488c-88de-4603f4e6e2d5",
   "metadata": {},
   "source": [
    "# 010 Gathering Data - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f99cc2-a6c2-47ce-987c-37a22e225ab5",
   "metadata": {},
   "source": [
    "**Author**: Andrew Yew Chean Yang <br>\n",
    "**Date**: 2023-04-11\n",
    "\n",
    "Table of contents are available for JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc880ff-50de-465f-9b36-89199bc75416",
   "metadata": {},
   "source": [
    "# Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df973e-b53c-452a-b7f7-3b3ae4ede79d",
   "metadata": {},
   "source": [
    "Given supply disruptions due to recent global events such as the Covid 19 pandemic and the war in Ukraine, Canadian food prices have risen at an alarming rate of [11% year on year in 2022](https://www150.statcan.gc.ca/n1/pub/62f0014m/62f0014m2022014-eng.htm), putting pressure on the average consumerâ€™s budget. This combined with the benefits of [home cooking](https://food-guide.canada.ca/en/healthy-eating-recommendations/cook-more-often/) has undoubtedly led to many busy working Canadians to cook more at home. However, assuming a busy work life, many adults need a way to ensure whatever they choose to cook is worth the precious time and effort after work. The recipe classifier seeks to address this issue for busy working adults by classifying recipes as worth the time and effort or not worth it, given the different elements available in online food recipes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332fd10-aa9d-4d9d-9934-be6c8c7686c8",
   "metadata": {},
   "source": [
    "# Notebook Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754bd88-310e-4f1d-9c43-27d483b80c2f",
   "metadata": {},
   "source": [
    "This notebook details the process of using requests and BeautifulSoup packages to gather data from food recipes in [allrecipes.com](https://www.allrecipes.com). A total of 40,001 recipes were scraped using the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d9bdb-effd-4cd2-8f15-b814bbb6b78d",
   "metadata": {},
   "source": [
    "Step 1: Gather initial list of recipes\n",
    "- As there was no single index that contained the uniform resourcee locator(url) of all recipes, one of the landing pages of allrecipes.com was chosen to initialize the recipe gathering process.\n",
    "\n",
    "Step 2: Scrape individual recipe webpages\n",
    "- Each of the webpages gathered from step 1, be it recipe webpages or landing pages, was scraped using requests.\n",
    "\n",
    "Step 3: Soupify response\n",
    "- The responses were converted into BeautifulSoup objects to gather specific parts of the response as some parts are not useful to the project, such as formatting information.\n",
    "\n",
    "Step 4: Store recipe contents into DataFrame\n",
    "- The scraped data was stored in a DataFrame, with 1 row representing 1 recipe\n",
    "\n",
    "Step 5: Extract links to other recipes\n",
    "- One feature of allrecipes.com webpages are that they contain links to other recipes. These links were extracted to find new recipes to further scrape.\n",
    "\n",
    "Repeat steps 2-5 until sufficient data collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f9825-f89a-4c32-98bf-0bdb3d512912",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "As the entire web scraping process is dependent on the live webpages at allrecipes.com and the recipes recommended within each recipe, the data collection process is time dependent and may produce different results at each scrape. The data for this study was collected in approximately 100 hours scraping over two weeks in the month of February. This notebook is a tidier recreation of the original notebook, with truncated lists to simulate the original run. To repeat the original run, iterations that involve scraping the 40,001 recipes must have their truncation filters removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af0c93-e5ae-4539-9e2a-c3d76d97a8a8",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "90ed2abc-a5ea-423a-aaf7-482dcbb64672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for array and data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# for sending and receiving url requests\n",
    "import requests\n",
    "\n",
    "# for cleaning up received http\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# for timing functions and diagnostics\n",
    "import time\n",
    "\n",
    "# for exiting the program\n",
    "import sys\n",
    "import joblib\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189bf3e-88a7-436f-9364-125d11cd6927",
   "metadata": {},
   "source": [
    "# Define Functions and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba5df266-7658-47e0-8cb3-ba0d34c02a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas option to display recipe urls in full for visual inspection\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebfed69e-7d59-4c01-9de3-cb03ce139a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url_list):\n",
    "    '''\n",
    "    This function will send and receive requests for each uniform resource locator(url) in a list.\n",
    "    \n",
    "    **Usage**\n",
    "    This function was used to gather even more recipe urls given a list of recipe urls.\n",
    "    \n",
    "    **Input**\n",
    "    urllist : a list containing urls to send and receive requests for\n",
    "    \n",
    "    **Output**\n",
    "    response_list : a list of gathered responses\n",
    "    '''\n",
    "    \n",
    "    # perform assertions for early error detection due to data type\n",
    "    assert isinstance(url_list, list), \"url_list should be a list of urls\"\n",
    "    for url in url_list:\n",
    "        assert isinstance(url, str), \"url should be a string\"\n",
    "    \n",
    "    # initiate empty response list\n",
    "    response_list = []\n",
    "    \n",
    "    # send and get a response for each url in list\n",
    "    for index, url in enumerate(url_list):\n",
    "        # Initiate timer\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        # Set cooldown time between requests\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Send a GET request to gather response\n",
    "        response = requests.get(url = url,\n",
    "                                allow_redirects = True)\n",
    "        \n",
    "        # End timer\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        # Statement to deal with unsuccessful responses (responses that are not 200)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Problem url at index: {index} for link: {url}\") \n",
    "            print(f\"Response code: {response.status_code}\")\n",
    "            print(f\"Response reason: {response.reason}\")\n",
    "            print(f\"Response encoding: {response.encoding}\")\n",
    "            exit()\n",
    "        else:\n",
    "            # Append the text from each response into response_list\n",
    "            response_list.append(response.text)\n",
    "        \n",
    "        # Print statement to check on progress through url_list\n",
    "        print(f\"{index+1} of {len(url_list)} done, time taken: {np.round(end-start)} seconds\", end='\\r')\n",
    "    \n",
    "    # return the response_list\n",
    "    return response_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9541f1bf-da6c-4c90-8894-9c2bddd110ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_response(response_list):\n",
    "    '''\n",
    "    This function takes the output from the send_request function and converts the response into a list of dictionaries using BeautifulSoup\n",
    "    \n",
    "    **Usage**\n",
    "    This function was used to soupify the output from the send_request function\n",
    "    \n",
    "    **Input**\n",
    "    response_list: a list of response_texts\n",
    "    \n",
    "    **Output**\n",
    "    soup_list: a list of converted(soupified) responses\n",
    "    '''\n",
    "    #Perform assertions for early error detection due to data type\n",
    "    assert isinstance(response_list, list), \"response_list should be a list of http responses\"\n",
    "    for response in response_list:\n",
    "        assert isinstance(response, str), \"response should be a string\"\n",
    "        \n",
    "    # Initiate blank list to store soupified response text\n",
    "    soup_list = []\n",
    "    \n",
    "    # Iterate through each response text in list\n",
    "    for response in response_list:\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        soup_list.append(soup)\n",
    "    \n",
    "    # Return soupified responses\n",
    "    return soup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f153d3-cc67-4699-a7a0-676277b24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link(soup_list, url_list, search_term):\n",
    "    '''\n",
    "    This function extracts recipe_urls from soupified responses. Specifically, 1 source recipe url may contain multiple recipe urls. This function converts the one to many relationship into a long table.\n",
    "    \n",
    "    **Usage**\n",
    "    This function was used to gather more recipe_urls to further scrape more data.\n",
    "    \n",
    "    **Input**\n",
    "    soup_list: The output from function 'convert_response'. A list of soupified responses.\n",
    "    url_list: A list of urls from which the responses were gathered from. The same url list used in the function 'send_request',\n",
    "    search_term: A string to identify recipe or specific types of urls from the soupified object.\n",
    "    \n",
    "    **Output**\n",
    "    extracted_url_df: A DataFrame containing extracted urls for further scraping along with the original urls they were scraped from.\n",
    "    '''\n",
    "    \n",
    "    # Perform assertions for early error detection in length of lists\n",
    "    assert len(soup_list) == len(url_list)\n",
    "    \n",
    "    # Initiate a blank DataFrame to store extracted links\n",
    "    extracted_url_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each soupified response\n",
    "    for index, soup in enumerate(soup_list):\n",
    "        \n",
    "        # Initiate a blank list to store extracted urls\n",
    "        extracted_url_list = []\n",
    "        \n",
    "        # Iterate through all url objects in the soupified response\n",
    "        for link in soup.find_all('a'):\n",
    "            \n",
    "            # Append each url oject to the blank list\n",
    "            extracted_url_list.append(link.get('href'))\n",
    "        \n",
    "        # Convert the url list into a DataFrame\n",
    "        temp_extracted_url_df = pd.DataFrame(extracted_url_list, columns = [\"extracted_url\"])\n",
    "        \n",
    "        # Keep only relevant urls based on specified search term\n",
    "        temp_extracted_url_df = temp_extracted_url_df[(temp_extracted_url_df['extracted_url'].str.contains('http'))\n",
    "                                                      &(temp_extracted_url_df['extracted_url'].str.contains(search_term))\n",
    "                                                     ].reset_index(drop = True)\n",
    "        \n",
    "        # Add the source url for back tracking if needed\n",
    "        temp_extracted_url_df['source_url'] = url_list[index]\n",
    "        \n",
    "        # Concatenate the extracted urls back to DataFrame\n",
    "        extracted_url_df = pd.concat([extracted_url_df, temp_extracted_url_df]).reset_index(drop = True)\n",
    "    \n",
    "    # Return all extracted DataFrame\n",
    "    return extracted_url_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a31c5-8f76-4aff-91b8-1d3b3af25165",
   "metadata": {},
   "source": [
    "# Initialize the Web Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874799ca-3bd8-45cc-973c-dca15c63eb89",
   "metadata": {},
   "source": [
    "As there was no single index containing a complete list of recipe urls, an initial list of recipes was used to kick start the web scraping process. From this initial list of recipes, more recipes can be gathered from recipes since each recipe contains promotional links to other recipes. \n",
    "\n",
    "To achieve this, the landing page of Allrecipes ingredients was chosen for the initial scrape: [Ingredients A-Z](https://www.allrecipes.com/ingredients-a-z-6740416) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d849ac-d420-47bb-aebe-00a4a994a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store landing page for Allrecipes ingredients in variable\n",
    "url_list = [\"https://www.allrecipes.com/ingredients-a-z-6740416\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cd10a6-1fa7-42c3-8218-2c036f2bcd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 1 done, time taken: 3.0 seconds\r"
     ]
    }
   ],
   "source": [
    "# Use predefined function to send a GET request to the url\n",
    "response_list = send_request(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270b14f7-0449-401d-9b14-0c1bc7669122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predefined function to soupify receivedd response\n",
    "soup_list = convert_response(response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8569e1-f866-442a-a7b0-5db508aac0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use predefined function to extract urls that contain recipes\n",
    "# The search term \"/recipes/\" was used to identify urls that act as landing pages for more recipes\n",
    "extracted_url_df = extract_link(soup_list, url_list, \"/recipes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3d71d4-9c57-4a72-8e82-28498236ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape from initial scrape: (168, 2).\n"
     ]
    }
   ],
   "source": [
    "# DataFrame shape from initial scrape\n",
    "print(f\"Shape from initial scrape: {extracted_url_df.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe45aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated urls: 56.\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicated urls\n",
    "print(f\"Number of duplicated urls: {extracted_url_df.duplicated().sum()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4442e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping duplicates: (112, 2).\n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicates to avoid web-scraping the same website twice\n",
    "extracted_url_df.drop_duplicates(inplace = True)\n",
    "print(f\"Shape after dropping duplicates: {extracted_url_df.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "098d8e84-eacf-474e-8b1b-61a0a4e30595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_url</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "      <td>https://www.allrecipes.com/ingredients-a-z-6740416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.allrecipes.com/recipes/17057/everyday-cooking/more-meal-ideas/5-ingredients/main-dishes/</td>\n",
       "      <td>https://www.allrecipes.com/ingredients-a-z-6740416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.allrecipes.com/recipes/15436/everyday-cooking/one-pot-meals/</td>\n",
       "      <td>https://www.allrecipes.com/ingredients-a-z-6740416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.allrecipes.com/recipes/1947/everyday-cooking/quick-and-easy/</td>\n",
       "      <td>https://www.allrecipes.com/ingredients-a-z-6740416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.allrecipes.com/recipes/455/everyday-cooking/more-meal-ideas/30-minute-meals/</td>\n",
       "      <td>https://www.allrecipes.com/ingredients-a-z-6740416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          extracted_url  \\\n",
       "0                                                      https://www.allrecipes.com/recipes/17562/dinner/   \n",
       "1  https://www.allrecipes.com/recipes/17057/everyday-cooking/more-meal-ideas/5-ingredients/main-dishes/   \n",
       "2                              https://www.allrecipes.com/recipes/15436/everyday-cooking/one-pot-meals/   \n",
       "3                              https://www.allrecipes.com/recipes/1947/everyday-cooking/quick-and-easy/   \n",
       "4              https://www.allrecipes.com/recipes/455/everyday-cooking/more-meal-ideas/30-minute-meals/   \n",
       "\n",
       "                                           source_url  \n",
       "0  https://www.allrecipes.com/ingredients-a-z-6740416  \n",
       "1  https://www.allrecipes.com/ingredients-a-z-6740416  \n",
       "2  https://www.allrecipes.com/ingredients-a-z-6740416  \n",
       "3  https://www.allrecipes.com/ingredients-a-z-6740416  \n",
       "4  https://www.allrecipes.com/ingredients-a-z-6740416  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the recipes from the initial scrape\n",
    "extracted_url_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c528535-76bc-43ee-a726-909984b38cd8",
   "metadata": {},
   "source": [
    "The initial scrape of recipes from the ingredients landing page at allrecipes.com did not yield any recipe urls to scrape. Instead, over 100 sub-landing pages were found, each leading to a list of recipes to web scrape. As such, the initialization of the web-scraping process was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66475864-35b8-4123-abc1-26e5cfcb3c24",
   "metadata": {},
   "source": [
    "# Gather List of Recipe URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097529d-46de-49b6-971e-7bd44574b2dd",
   "metadata": {},
   "source": [
    "Now that a list of landing pages to gather recipe urls from has been established, an iterative process was used to gather a list of recipe urls from those landing pages and other recipe pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ea53dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a blank DataFrame to store recipes\n",
    "recipe_url_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd11fa-e652-4706-b3b9-d8fb912e7acf",
   "metadata": {},
   "source": [
    "Similar to the initial web scrape, the below process was repeated for each iteration of data gathering:\n",
    "- send_request\n",
    "- convert_response\n",
    "- extract_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58bce5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new url list using the extracted urls from the initial scrape\n",
    "url_list = extracted_url_df['extracted_url'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "268ba32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 of 112 done, time taken: 3.0 seconds\r"
     ]
    }
   ],
   "source": [
    "# Use predefined function to send a GET request to the url\n",
    "response_list = send_request(url_list)\n",
    "\n",
    "# Use predefined function to soupify received response\n",
    "soup_list = convert_response(response_list)\n",
    "\n",
    "# Use predefined function to extract urls that contain recipes\n",
    "# The search term \"/recipe/\" (singular) was used to identify recipe urls\n",
    "temp_df = extract_link(soup_list,url_list,\"/recipe/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "953b00eb-77cd-4497-bd6c-e73ac4209314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipe urls gathered: 6377.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of recipe urls gathered: {temp_df.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48d937ff-cedd-4a5a-b90a-4e343aadda09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique recipe urls gathered: 4618.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated recipes to avoid scraping the same recipe twice\n",
    "temp_df.drop_duplicates(subset = ['extracted_url'], inplace = True)\n",
    "print(f\"Number of unique recipe urls gathered: {temp_df.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "002f9dea-1f14-4d76-91c4-8770e93e3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the gathered recipe urls into recipe_url_df\n",
    "recipe_url_df = pd.concat([recipe_url_df,temp_df],axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14a62078-6a19-49fc-bccc-dbc687b38b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique recipe urls gathered: 4618.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_url</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.allrecipes.com/recipe/83646/corned...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.allrecipes.com/recipe/158799/stout...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.allrecipes.com/recipe/8509102/chic...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.allrecipes.com/recipe/8508920/miss...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.allrecipes.com/recipe/255462/lasag...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       extracted_url  \\\n",
       "0  https://www.allrecipes.com/recipe/83646/corned...   \n",
       "1  https://www.allrecipes.com/recipe/158799/stout...   \n",
       "2  https://www.allrecipes.com/recipe/8509102/chic...   \n",
       "3  https://www.allrecipes.com/recipe/8508920/miss...   \n",
       "4  https://www.allrecipes.com/recipe/255462/lasag...   \n",
       "\n",
       "                                         source_url  \n",
       "0  https://www.allrecipes.com/recipes/17562/dinner/  \n",
       "1  https://www.allrecipes.com/recipes/17562/dinner/  \n",
       "2  https://www.allrecipes.com/recipes/17562/dinner/  \n",
       "3  https://www.allrecipes.com/recipes/17562/dinner/  \n",
       "4  https://www.allrecipes.com/recipes/17562/dinner/  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Number of unique recipe urls gathered: {recipe_url_df.shape[0]}.\")\n",
    "display(recipe_url_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8ae1a-9fc4-4b6f-8bc0-fd005f24db42",
   "metadata": {},
   "source": [
    "Using the extracted landing pages from the initial scrape, about 4,600 recipe urls were gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5428ee4e-e9f6-4acd-8151-982b3a3c7312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landing pages scraped: 14310.\n"
     ]
    }
   ],
   "source": [
    "# From the same scrape, examine if any new landing pages were extracted\n",
    "temp_df2 = extract_link(soup_list,url_list,\"/recipes/\")\n",
    "\n",
    "print(f\"Landing pages scraped: {temp_df2.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b14e3b-b899-46d3-ba4b-c2c1f9e8962f",
   "metadata": {},
   "source": [
    "14 thousand landing pages appears to be too much given we started with 1. Duplicate landing pages were identified and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e45912c-ce2a-4cf6-9aeb-771c9f6a2032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique recipe urls gathered: 902.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate landing pages\n",
    "temp_df2.drop_duplicates(subset = ['extracted_url'], inplace = True)\n",
    "print(f\"Number of unique recipe landing pages gathered: {temp_df2.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e14f9-fd25-45b5-92f5-ad22925bc568",
   "metadata": {},
   "source": [
    "Finally, for the next round of scraping, the above 900 urls were checked against previously scraped landing pages to avoid repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b449f17f-5a24-453c-9f3a-b512b5823e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previously scraped landing pages: 111.\n",
      "Number of unique new recipe urls gathered: 791.\n"
     ]
    }
   ],
   "source": [
    "# Define condition to check for previously scraped landing pages\n",
    "cond1 = temp_df2['extracted_url'].isin(extracted_url_df['extracted_url'])\n",
    "print(f\"Number of previously scraped landing pages: {cond1.sum()}.\")\n",
    "\n",
    "# Apply condition to remove all duplicates\n",
    "temp_df2 = temp_df2[~cond1]\n",
    "print(f\"Number of unique new recipe landing pages gathered: {temp_df2.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e74eea0c-d4d5-4b6f-816d-1968d163dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store unique new landing pages into a DataFrame\n",
    "new_landing_pages = temp_df2.copy()\n",
    "\n",
    "# Add in new landing pages into previous DataFrame of landing pages for reference\n",
    "extracted_url_df = pd.concat([extracted_url_df,new_landing_pages], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4af695-81e0-403a-9708-abe570135168",
   "metadata": {},
   "source": [
    "Thus, to summarize the first round of gathering recipes, a total of approximately 4,600 recipes and 800 new recipe landing pages were gathered. This meant for the next round, a total of 5,400 requests will be sent out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce71bf1a",
   "metadata": {},
   "source": [
    "# Gather List of Recipe URLs (2nd iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c925df8-5ca6-4879-bb19-9ab8c8f986b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls to scrape : 5409\n"
     ]
    }
   ],
   "source": [
    "# Initiate list of urls from which to scrape\n",
    "url_list = new_landing_pages['extracted_url'].to_list() + recipe_url_df['extracted_url'].to_list()\n",
    "print(f\"Number of urls to scrape : {len(url_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ef77d4a-c192-4641-a019-6aa9fc924acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5409 of 5409 done, time taken: 1.0 seconds\r"
     ]
    }
   ],
   "source": [
    "# Use predefined function to send a GET request to the url\n",
    "response_list = send_request(url_list)\n",
    "\n",
    "# Use predefined function to soupify received response\n",
    "soup_list = convert_response(response_list)\n",
    "\n",
    "# Use predefined function to extract urls that contain recipes\n",
    "# The search term \"/recipe/\" (singular) was used to identify recipe urls\n",
    "temp_df = extract_link(soup_list,url_list,\"/recipe/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1aa3277d-351e-4d10-a29b-988d25078609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipe urls gathered: 114520.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of recipe urls gathered: {temp_df.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e49bad11-9d5c-4f57-8111-5eb5c854eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique recipe urls gathered: 28350.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated recipes to avoid scraping the same recipe twice\n",
    "temp_df.drop_duplicates(subset = ['extracted_url'], inplace = True)\n",
    "print(f\"Number of unique recipe urls gathered: {temp_df.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f059c917-fb7a-4fa3-bb5d-90474e603b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previously scraped recipes: 0.\n",
      "Number of unique new recipe urls gathered: 24256.\n"
     ]
    }
   ],
   "source": [
    "# Define condition to check for previously scraped recipes\n",
    "cond1 = temp_df['extracted_url'].isin(recipe_url_df['extracted_url'])\n",
    "print(f\"Number of previously scraped recipes: {cond1.sum()}.\")\n",
    "\n",
    "# Apply condition to remove all duplicates\n",
    "temp_df = temp_df[~cond1]\n",
    "print(f\"Number of unique new recipe urls gathered: {temp_df.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63bf6a38-b040-4174-b974-88b5ee652621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the gathered recipe urls into recipe_url_df\n",
    "recipe_url_df = pd.concat([recipe_url_df,temp_df],axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ca038b1-1394-48a2-989f-dfd15a8e0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique recipe urls gathered: 28874.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_url</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.allrecipes.com/recipe/83646/corned...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.allrecipes.com/recipe/158799/stout...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.allrecipes.com/recipe/8509102/chic...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.allrecipes.com/recipe/8508920/miss...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.allrecipes.com/recipe/255462/lasag...</td>\n",
       "      <td>https://www.allrecipes.com/recipes/17562/dinner/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       extracted_url  \\\n",
       "0  https://www.allrecipes.com/recipe/83646/corned...   \n",
       "1  https://www.allrecipes.com/recipe/158799/stout...   \n",
       "2  https://www.allrecipes.com/recipe/8509102/chic...   \n",
       "3  https://www.allrecipes.com/recipe/8508920/miss...   \n",
       "4  https://www.allrecipes.com/recipe/255462/lasag...   \n",
       "\n",
       "                                         source_url  \n",
       "0  https://www.allrecipes.com/recipes/17562/dinner/  \n",
       "1  https://www.allrecipes.com/recipes/17562/dinner/  \n",
       "2  https://www.allrecipes.com/recipes/17562/dinner/  \n",
       "3  https://www.allrecipes.com/recipes/17562/dinner/  \n",
       "4  https://www.allrecipes.com/recipes/17562/dinner/  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Number of unique recipe urls gathered: {recipe_url_df.shape[0]}.\")\n",
    "display(recipe_url_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e8044f5-d884-4d42-82fd-2781a4a12374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landing pages scraped: 571592.\n"
     ]
    }
   ],
   "source": [
    "# From the same scrape, examine if any new landing pages were extracted\n",
    "temp_df2 = extract_link(soup_list,url_list,\"/recipes/\")\n",
    "\n",
    "print(f\"Landing pages scraped: {temp_df2.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0922a1b-6785-45a2-aabc-f0a4373bfe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique recipe landing pages gathered: 2290.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate landing pages\n",
    "temp_df2.drop_duplicates(subset = ['extracted_url'], inplace = True)\n",
    "print(f\"Number of unique recipe landing pages gathered: {temp_df2.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c80f1ec2-ec96-4319-be2b-ea9807147b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previously scraped landing pages: 901.\n",
      "Number of unique new recipe landing pages gathered: 1389.\n"
     ]
    }
   ],
   "source": [
    "# Define condition to check for previously scraped landing pages\n",
    "cond1 = temp_df2['extracted_url'].isin(extracted_url_df['extracted_url'])\n",
    "print(f\"Number of previously scraped landing pages: {cond1.sum()}.\")\n",
    "\n",
    "# Apply condition to remove all duplicates\n",
    "temp_df2 = temp_df2[~cond1]\n",
    "print(f\"Number of unique new recipe landing pages gathered: {temp_df2.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60e574aa-fdfd-42c1-8217-13989c384fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store unique new landing pages into a DataFrame\n",
    "new_landing_pages = temp_df2.copy()\n",
    "\n",
    "# Add in new landing pages into previous DataFrame of landing pages for reference\n",
    "extracted_url_df = pd.concat([extracted_url_df,new_landing_pages], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31bcd2-1bb5-4b7d-bd51-82ce37724818",
   "metadata": {},
   "source": [
    "At the second iteration, a total of approximately 24,000 new recipe urls and 1,400 new landing pages were gathered. Although not shown in this notebook due to time constraints, the above steps were iterated one more time to arrive at the final dataset of 40,001 recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8964f2f3-1e40-4158-8044-3312b9d5bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save recipe url df as a pickle file\n",
    "joblib.dump(recipe_url_df['extracted_url'], 'data/recipe_url_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c43c80-6df8-4c55-a697-c12f5f819e6c",
   "metadata": {},
   "source": [
    "# Scrape Data from Recipe URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62c40c-54c3-4e6d-963b-6038c8901f14",
   "metadata": {},
   "source": [
    "Now that a list of recipe urls have been gathered, the next step is to iterate through each recipe url and scrape the data of each recipe. To identify what data is available from each recipe website and how to scrape the specific data, the 'inspect element' function of the Google Chrome web browser was used. Note that elements of a website can be inspected using a web browser of your choice.\n",
    "\n",
    "The recipe webpage's elements were inspected to identify JSON dictionary key pairs that can be used with BeautifulSoup to extract the specific data. Note as most of a webpage's elements consists of design elements, this method of specifically targetting parts of the webpage to store reduces the size of the data gathered and keeps data gathered as simple as possible.\n",
    "\n",
    "The beneath for loop goes through each recipe url and extracts 17 elements from each website that were deemed useful at this stage of the project. Note that each element has a try and except clause to deal with cases where the specified element is not present in the webpage. Regular expression was used with BeautifulSoup to extract specific elements of data.\n",
    "\n",
    "Finally, a pause was set between each response to avoid sending too many requests consecutively too quickly, which may lead to allrecipes blocking the IP address of the local computer. As such, below code took appoximately 84 hours to run in entirety. (40,000 recipe url * 7.5seconds per url = 300,000 seconds or approximately 84 hours.) Below code appears to not be ran before as this was a tidied up version from the original notebook used to scrape the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4e488-4b3f-4d37-9992-f3f7f5d576e9",
   "metadata": {},
   "source": [
    "| Column_number | Column_name            | Data Type  | Description                                                                        |\n",
    "|---------------|------------------------|------------|------------------------------------------------------------------------------------|\n",
    "| 1             | url                    | string     | the url of the recipe                                                              |\n",
    "| 2             | title                  | string     | the title of the recipe                                                            |\n",
    "| 3             | image                  | list       | any image urls found within the recipe                                             |\n",
    "| 4             | rating_average         | string     | average of ratings, the target feature                                             |\n",
    "| 5             | rating_count           | string     | the number of ratings for the recipe                                               |\n",
    "| 6             | review_count           | string     | the number of reviews for the recipe                                               |\n",
    "| 7             | description            | string     | the description section beneath each title of the recipe                           |\n",
    "| 8             | update_date            | string     | the last date of update for the recipe                                             |\n",
    "| 9             | ingredient             | list       | a list of ingredients and their amounts                                            |\n",
    "| 10            | direction              | list       | a list of cooking directions or instructions                                       |\n",
    "| 11            | nutrition_summary      | dictionary | a dictionary of nutritional information summary                                    |\n",
    "| 12            | nutrition_detail       | dictionary | a dictionary of detailed nutritional information                                   |\n",
    "| 13            | time                   | list       | a dictionary containing time related values in the recipe                          |\n",
    "| 14            | label                  | list       | a list containing the labels or tags associated with the recipe                    |\n",
    "| 15            | review_dict            | dictionary | a JSON dictionary object containing reviews and other data elements of the webpage |\n",
    "| 16            | description_additional | list       | additional description if available for the recipe                                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e0f03a71-c3ce-45b8-bde5-f17b88734ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = recipe_url_df['extracted_url'][0:3].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fb35e225-4ee1-454e-9ef1-bd2913d2bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 of 3 done, time taken: 6.0 seconds.\r"
     ]
    }
   ],
   "source": [
    "# Store the number of recipe urls to be scraped into a variable for progress status printing\n",
    "# Only the first 10 rows were run for demonstration purposes\n",
    "number_of_url = len(url_list)\n",
    "\n",
    "# Initiate a blank DataFrame to store the scraped data\n",
    "raw_data_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each recipe url\n",
    "for index, recipe_url in enumerate(url_list):\n",
    "    \n",
    "    # Send a get request for the recipe url and receive a response\n",
    "    try:\n",
    "        # Initiate timer\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        # Use random integer to set cooldown time between scrapes\n",
    "        time.sleep(np.random.randint(low = 5,high = 10))\n",
    "        \n",
    "        # Store the response in a variable\n",
    "        response = requests.get(url = recipe_url, allow_redirects = True)\n",
    "        \n",
    "        # End timer\n",
    "        end = time.perf_counter()\n",
    "    \n",
    "    # Except clause to deal with problem urls\n",
    "    except:\n",
    "        print(f\"Problem url at {index+1} of {number_of_url}\") \n",
    "        print(f\"Response code: {response.status_code}\")\n",
    "        print(f\"Response reason: {response.reason}\")\n",
    "        print(f\"Response encoding: {response.encoding}\")\n",
    "        exit()\n",
    "    \n",
    "    \n",
    "    # Convert response to soup object\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except:\n",
    "        print(f\"soup_error at {index+1} of {number_of_url}\")\n",
    "    \n",
    "    \n",
    "    # Initiate a blank dictionary to store values\n",
    "    temp_dict = dict()\n",
    "    \n",
    "    # column 00: url, the url of the recipe\n",
    "    try:\n",
    "        temp_dict.update({\"recipe_url\": recipe_url})\n",
    "    except:\n",
    "        temp_dict.update({\"recipe_url\": np.NaN})\n",
    "        print(f\"url_error at {index+1} of {number_of_url}\")\n",
    "    \n",
    "    \n",
    "    # column 01: title, the title of the recipe\n",
    "    try:\n",
    "        temp_dict.update({\"title\":\n",
    "                          soup.find(\"h1\", {\"id\": re.compile(\"^article-heading_*\")}).get_text().strip(' \\t\\n\\r')\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"title\": np.NaN})\n",
    "        print(f\"title_error at {index+1} of {number_of_url}\")\n",
    "\n",
    "        \n",
    "    # column 02: image, any image urls found within the recipes\n",
    "    try:\n",
    "        t_main_img = [img.get(\"src\") for img in soup.find(\"div\", {\"class\": \"loc article-content\"}).find_all(\"img\") if img.get(\"src\") != \"\"]\n",
    "        t_sub_img = [img.get(\"data-src\") for img in soup.find(\"div\", {\"class\": \"loc article-content\"}).find_all(\"img\") if img.get(\"data-src\") != None]\n",
    "        t_img = list((set(t_main_img+t_sub_img)))\n",
    "        temp_dict.update({\"image\":t_img})\n",
    "    except:\n",
    "        temp_dict.update({\"image\": np.NaN})\n",
    "        print(f\"image_error at {index+1} of {number_of_url}\")\n",
    "\n",
    "    \n",
    "    # column 03: rating_average, the target feature\n",
    "    try:\n",
    "        temp_dict.update({\"rating_average\":\n",
    "                          float(soup.find(\"div\", {\"id\": re.compile(\"mntl-recipe-review-bar__rating_*\")}).get_text().strip(' \\t\\n\\r'))\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"rating_average\": np.NaN})\n",
    "        print(f\"rating_average_error at {index+1} of {number_of_url}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    # column 04: rating_count, the number of ratings for the recipe\n",
    "    try:\n",
    "        temp_dict.update({\"rating_count\":\n",
    "                          soup.find(\"div\", {\"id\": re.compile(\"^mntl-recipe-review-bar__rating-count_*\")}).get_text().strip(' \\t\\n\\r()')\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"rating_count\": np.NaN})\n",
    "        print(f\"rating_count_error at {index+1} of {number_of_url}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    # column 05: review_count, the number of reviews for the recipe\n",
    "    try:\n",
    "        temp_dict.update({\"review_count\":\n",
    "                         soup.find(\"div\", {\"id\": re.compile(\"^mntl-recipe-review-bar__comment-count_*\")}).get_text().strip(' \\t\\n\\r()')\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"review_count\": np.NaN})\n",
    "        print(f\"review_count_error at {index+1} of {number_of_url}\")\n",
    "\n",
    "        \n",
    "    # column 06: description, the description section beneath each title of the recipe\n",
    "    try:\n",
    "        temp_dict.update({\"description\":\n",
    "                         soup.find(\"p\", {\"id\" : re.compile(\"^article-subheading_*\")}).get_text().strip(' \\t\\n\\r')\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"description\": np.NaN})\n",
    "        print(f\"description_error at {index+1} of {number_of_url}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    # column 07: update_date, the last date of update for the recipe\n",
    "    try:\n",
    "        temp_dict.update({\"update_date\":\n",
    "                         soup.find_all(\"div\", {\"class\": re.compile(\"^mntl-attribution__item-date*\")})[0].get_text()\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"update_date\": np.NaN})\n",
    "        print(f\"update_date_error at {index+1} of {number_of_url}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    # column 08: ingredient, a list of ingredients and their amounts\n",
    "    try:\n",
    "        temp_dict.update({\"ingredient\":\n",
    "                         [li.get_text().strip(' \\t\\n\\r') for li in soup.find(\"div\", {\"id\": re.compile(\"^mntl-structured-ingredients_*\")}).find_all(\"li\")]\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"ingredient\": np.NaN})\n",
    "        print(f\"ingredient_error at {index+1} of {number_of_url}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    # column 09: direction, a list of cooking directions or instructions\n",
    "    try:\n",
    "        temp_dict.update({\"direction\":\n",
    "                          [li.get_text().strip(' \\t\\n\\r') for li in soup.find(\"div\", {\"id\": re.compile(\"^recipe__steps-content_*\")}).find_all(\"li\")]\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"direction\": np.NaN})\n",
    "        print(f\"direction_error at {index+1} of {number_of_url}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    # column 10: nutrition_summary, a dictionary of nutritional information summary\n",
    "    try:\n",
    "        tag = soup.find(\"div\", {\"id\": re.compile(\"^mntl-nutrition-facts-summary_*\")})\n",
    "        \n",
    "        t_value = [line.get_text() for line in tag.find_all(\"td\",{\"class\":\"mntl-nutrition-facts-summary__table-cell type--dog-bold\"})]\n",
    "        header_1 = [line.get_text() for line in tag.find_all(\"td\",{\"class\":\"mntl-nutrition-facts-summary__table-cell type--dogg\"})]\n",
    "        header_2 = [line.get_text() for line in tag.find_all(\"td\",{\"class\":\"mntl-nutrition-facts-summary__table-cell type--dog\"})]\n",
    "        t_header = header_1+header_2\n",
    "        \n",
    "        temp_dict.update({\"nutrition_summary\":\n",
    "                          {key:value for (key,value) in zip(t_header,t_value)}\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"nutrition_summary\": np.NaN})\n",
    "        print(f\"nutrition_summary_error at {index+1} of {number_of_url}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    # column 11: nutrition_detail, a dictionary of detailed nutritional information\n",
    "    try:\n",
    "        temp_dict.update({\"nutrition_detail\":\n",
    "                          pd.read_html(str(soup.find_all(\"table\",{\"class\": \"mntl-nutrition-facts-label__table\"})))[0]\\\n",
    "                          .iloc[:,0].to_list()\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"nutrition_detail\": np.NaN})\n",
    "        print(f\"nutrition_detail_error at {index+1} of {number_of_url}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # column 12: time, a dictionary containing time related values in the recipe\n",
    "    try:\n",
    "        t_value = [div.get_text().strip(' \\t\\n\\r') for div in soup.find(\"div\", {\"id\": re.compile(\"^recipe-details_*\")}).find_all(\"div\", {\"class\":re.compile(\"^mntl-recipe-details__val*\")})]\n",
    "        t_header = [div.get_text().strip(' \\t\\n\\r') for div in soup.find(\"div\", {\"id\": re.compile(\"^recipe-details_*\")}).find_all(\"div\", {\"class\":re.compile(\"^mntl-recipe-details__la*\")})]\n",
    "        temp_dict.update({\"time\":\n",
    "                          {key:value for (key,value) in zip(t_header,t_value)}\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"time\": np.NaN})\n",
    "        print(f\"time_error at {index+1} of {number_of_url}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    # column 13: label, a list containing the labels or tags associated with the recipe\n",
    "    try:\n",
    "        temp_dict.update({\"label\":\n",
    "                        [label.get_text() for label in soup.find(\"div\", {\"class\":re.compile(\"^loc article-header\")}).find_all(\"span\",{\"class\":\"link__wrapper\"})]\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"label\": np.NaN})\n",
    "        print(f\"label_error at {index+1} of {number_of_url}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    # column 14: review_dict, dictionary containing a JSON dictionary of reviews and other data elements of the webpage\n",
    "    try:\n",
    "        temp_dict.update({\"review_dict\":\n",
    "                         ast.literal_eval(\n",
    "                             soup.find('script',{\"class\":\"comp allrecipes-schema mntl-schema-unified\"}).text\n",
    "                         )})\n",
    "    except:\n",
    "        temp_dict.update({\"review_dict\": np.NaN})\n",
    "        print(f\"review_dict_error at {index+1} of {number_of_url}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # column 15: description_additional, additional description if available for the recipe\n",
    "    try:\n",
    "        temp_dict.update({\"description_additional\":\n",
    "                         [p.get_text().strip(' \\t\\n\\r') for p in soup.find_all('p',{\"class\":re.compile(\"^mntl-sc-block*\")})]\n",
    "                         })\n",
    "    except:\n",
    "        temp_dict.update({\"description_additional\": np.NaN})\n",
    "        print(f\"description_additional_error at {index+1} of {number_of_url}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create a DataFrame with 1 row using the above data scraped into temp_dict\n",
    "    temp_df = pd.DataFrame({k: pd.Series([v]) for k,v in temp_dict.items()})\n",
    "    \n",
    "    # Concatenate the DataFrame with 1 row with the raw_data_df\n",
    "    raw_data_df = pd.concat([raw_data_df,temp_df],ignore_index= True, axis = 0)\n",
    "    \n",
    "    # Progress check\n",
    "    print(f\"{index+1} of {number_of_url} done, time taken: {np.round(end-start)} seconds.\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e4c24-ee17-4617-9db2-08536e2b5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save recipe url df as a pickle file\n",
    "joblib.dump(raw_data_df, 'data/raw_data_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99141ddd-3d06-4ff2-82be-23addbfe95c1",
   "metadata": {},
   "source": [
    "# Conclusion for Notebook 010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bc6d2-8a75-4f5c-a704-a6a77bc53d4a",
   "metadata": {},
   "source": [
    "To summarize, this notebook goes through the process of :\n",
    "- Gathering an initial list of recipe urls to scrape from a single landing page.\n",
    "- The feedback loop of using the initial list of recipe urls to scrap more recipe urls\n",
    "- The process of gathering data for each of the gathered recipe urls using BeautifulSoup and regular expressions\n",
    "\n",
    "Data for 40,001 recipes were gathered for the next notebook, which will detail the exploratory analysis and feature engineering of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
